'''
author: @callum
stats.py - some functions to do statistics, both on the
genomes and the outputs. not finished yet. need to think a little more

each of the averaging functions should return a tuple of a 'result'
and a list of output files it created. these can be empty, but they should be
there.
then do_files calls the set of functions, runs the statistics we want, and
returns a dict of outputs sorted by key, and a list of output files created
'''

import os
import sys
import re
import ast
import glob
import dataclasses
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from functools import reduce
import constants
import plots
import utils
import genetic_algorithm as ga
import rc as rcm

def combined_populations(output_dir):
    '''
    find the output final populations, combine them and return one
    combined dataframe which can then be fed into the other functions here
    '''
    files = glob.glob(os.path.join(output_dir, "*_final_population.csv"))
    n = len(files)
    if n != constants.n_runs:
        print(f"stats.combined_populations found {n} files: {[files]}")
    if n == 0:
        return None
    else:
        dfs = [pd.read_csv(f) for f in files]
        combined = pd.concat(dfs)
    return combined

def split_population(df, split=None):
    '''
    split the population found in dataframe df based on the split key.
    return a dict where the keys are the values of the split key, and the
    values are dataframes with the relevant subpopulations, i.e.
    df == pd.concat([v for v in df_dict.values()])
    '''
    if split is not None:
        # get unique values of the split key and make a dict from them
        df_dict = {rct: df[df[split] == rct] for rct in df[split].unique()}
    else:
        df_dict = {'all': df}
    return df_dict

def generate_genomes(df, **kwargs):
    '''
    split the population found in df if necessary, and then pull out the
    genome parameters for each (sub)population and instantiate them as Genomes
    again. return the dict generated by split_population and the
    subpopulations of Genomes as a list
    '''
    split = kwargs['split'] if 'split' in kwargs else None
    df_dict = split_population(df, split)
    subpops = []
    for sk, subdf in df_dict.items():
        subpop = []
        for i in range(len(subdf)):
            row = subdf.iloc[i]
            d = {}
            for index in row.index:
                # NB: this assumes that the genome corresponding to the
                # pickled population is the same as the current one, so
                # if you change the definition of the genome in
                # genetic_algorithm.py and then read in an old population,
                # it'll break. unsure how to get around this. in theory
                # i could save a minimal version of the genome to JSON?
                if index in ga.genome_parameters:
                    if ga.genome_parameters[index]['array']:
                        v = parse_array(row[index])
                    else:
                        v = row[index]
                    d[index] = v
            g = ga.Genome(**d)
            subpop.append(g)
        subpops.append(subpop)
    return df_dict, subpops

def avg(df, key, **kwargs):
    '''
    straight average and standard error of a parameter
    '''
    return (df[key].mean(), df[key].sem()), []

def counts(df, key, **kwargs):
    '''
    values and counts of a parameter (categorical stuff like RCs)
    '''
    res = df.value_counts(key)
    return (res.index.to_numpy(), res.values), []

def parse_array(string):
    '''
    parse a multidimensional array that's been saved as a string
    by pd.to_csv(). this is probably a bit finicky and i'm not sure it'll
    be consistent, but pickle doesn't work because of some arcane thing
    to do with how i define the genome, so we're stuck with it for now
    '''
    if isinstance(string, np.ndarray):
        return string
    else:
        # remove leading or trailing whitespace between brackets
        s = re.sub(r'\[\s+([0-9])', r'[\1', string)
        print(s)
        s = re.sub(r'([0-9])\s+\]', r'\1]', s)
        print(s)
        # remove the newlines and replace whitespace with commas for ast
        s = re.sub('\s+', ',', re.sub('\n ', ',', s))
        print(s)
        return np.array(ast.literal_eval(s))

def element_avg(df, key, **kwargs):
    '''
    elementwise average of an array. Note that the shape of the array
    must be constant across the whole population, so this will work for
    e.g. the redox states of each type of photosynthesis, so long as you
    split it up, but won't work for a mix of types, or for Genome-specific
    arrays like the number of subunits
    '''
    output = {}
    split = kwargs['split'] if 'split' in kwargs else None
    df_dict = split_population(df, split)
    for sk, subdf in df_dict.items():
        # check the shapes of the arrays are all the same
        a = parse_array(subdf.iloc[0][key])
        arr = np.zeros((len(subdf), *a.shape))
        shape = arr.shape
        for i in range(len(subdf)):
            b = parse_array(subdf.iloc[i][key])
            assert(a.shape == b.shape)
            arr[i] = b
        avg = np.mean(arr, axis=0)
        err = np.std(arr, axis=0) / np.sqrt(len(subdf))
        output[sk] = (avg, err)
    return output, []

def avg_sum(df, key, **kwargs):
    '''
    return an average of the sum of an array parameter. note that
    this must be a 1d array (because of the call to np.fromstring below,
    which will only parse a 1d array) but should be agnostic to the
    array lengths, unlike element_avg above
    '''
    output = {}
    split = kwargs['split'] if 'split' in kwargs else None
    df_dict = split_population(df, split)
    for sk, subdf in df_dict.items():
        arr = np.zeros(len(subdf))
        for i in range(len(subdf)):
            a = parse_array(subdf.iloc[i][key])
            arr[i] = np.sum(a)
            if key == 'n_p':
                arr[i] *= subdf.iloc[i]['n_b']
        avg = np.mean(arr)
        err = np.std(arr) / np.sqrt(len(subdf))
        output[sk] = (avg, err)
    return output, []

def hist(df, prefix, key, split=None, **kwargs):
    '''
    use pandas and seaborn to plot per-element
    histograms of array parameters
    '''
    # this is very inefficient. but basically, do one histogram for
    # each element of the given array, knowing that the arrays might be
    # different sizes (e.g. the redox and recombination arrays)
    hmax = np.max([len(np.array(df[key][i]).flatten())
        for i in range(len(df))])
    if key in ga.genome_parameters:
        if ga.genome_parameters[key]['array']:
            if ga.genome_parameters[key]['depends'] == "n_s":
                hmax = constants.hist_sub_max
    if isinstance(np.array(df[key][0]).flatten()[0], int):
        discrete = True
    else:
        discrete = False
    # figure out a layout for the plots
    if hmax % 2 == 0:
        nrows = hmax//2
        ncols = 2
    else:
        nrows = hmax
        ncols = 1
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols,
            figsize=(12 * ncols, 8 * nrows))
    for i in range(hmax):
        col = np.full(len(df), np.nan)
        for j in range(len(df)):
            v = np.array(df[key][j]).flatten()
            if i < len(v):
                col[j] = v[i]
        # add col to the dataframe so that we can plot it with seaborn
        str_id = f"{key}[{i}]"
        df[str_id] = col
        if hmax == 1:
            curr_ax = axes # otherwise the histplot will fail below
        elif hmax > 1 and (ncols == 1 or nrows == 1):
            curr_ax = axes[i] # otherwise the histplot will fail below
        else:
            curr_ax = axes[i // 2, i % 2]
        sns_kwargs = {'ax': curr_ax, 'data': df, 'x': str_id,
                'discrete': discrete}
        if split is not None:
            sns_kwargs['hue'] = split
            sns_kwargs['multiple'] = 'dodge'
        # sometimes seaborn will get confused trying to make the
        # histogram, which i think is a bin issue, so here try to
        # give it a sensible bin width to work with
        bw = (np.nanmax(col) - np.nanmin(col)) / 50
        if bw > 0.0 and bw != np.nan:
            sns_kwargs['binwidth'] = bw
        sns.histplot(**sns_kwargs)
        # delete the temporary index now we don't need it
        df.drop(columns=str_id, inplace=True)
    if split is not None:
        suffix = f"hist_{key}_split_by_{split}.pdf"
    else:
        suffix = f"hist_{key}.pdf"
    outfile = f"{prefix}_{suffix}"
    plt.savefig(outfile)
    plt.close()
    return (), [outfile]

def absorption(df, spectrum, prefix, **kwargs):
    '''
    convert a dataframe to a population of genomes and plot
    the average absorption of that population, potentially
    split into subpopulations using kwarg split. the argument key
    is only here for consistency with the other functions.
    '''
    output = {}
    outfiles = []
    split = kwargs['split'] if 'split' in kwargs else None
    df_dict, subpops = generate_genomes(df, **kwargs)
    for (rct, subdf), subpop in zip(df_dict.items(), subpops):
        abs_file = f"{prefix}_{rct}_abs.txt"
        output[rct], ofs = plots.plot_average(subpop, spectrum, abs_file)
        outfiles.extend(ofs)
    return output, outfiles

def ss_efficiency(df, spectrum, prefix, **kwargs):
    '''
    calculate the steady-state efficiency for a given population,
    split up if necessary as elsewhere. we define the steady-state
    efficiency as $ \nu_e / \sum \gamma $ where $ \gamma $ is the vector
    of photon inputs for a given antenna-RC supercomplex, i.e. it is the
    proportion of absorbed light that is successfully converted to electrons
    '''
    output = {}
    outfiles = []
    l = spectrum[:, 0]
    fp_y = (spectrum[:, 1] * l) / utils.hcnm
    split = kwargs['split'] if 'split' in kwargs else None
    df_dict, subpops = generate_genomes(df, **kwargs)
    for (rct, subdf), subpop in zip(df_dict.items(), subpops):
        ss_eff = np.zeros(len(subpop))
        rcp = rcm.params[rct]
        n_rc = len(rcp["pigments"])
        rc_n_p = [constants.pigment_data[rc]["n_p"] for rc in rcp["pigments"]]
        for j, p in enumerate(subpop):
            nu_e = subdf.iloc[j]['nu_e']
            n_p = np.array([*rc_n_p, *p.n_p], dtype=np.int32)
            shift = np.array([*[0.0 for _ in range(n_rc)], *p.shift],
                             dtype=np.float64)
            shift *= constants.shift_inc
            pigment = np.array([*rcp["pigments"], *p.pigment], dtype='U10')
            a_l = np.zeros((p.n_s + n_rc, len(spectrum[:, 0])))
            gamma = np.zeros(p.n_s + n_rc, dtype=np.float64)
            for i in range(p.n_s + n_rc):
                a_l[i] = utils.absorption(l, pigment[i], shift[i])
                gamma[i] = (n_p[i] * constants.sig_chl *
                        utils.overlap(l, fp_y, a_l[i]))
            sumg = np.sum(gamma[0:n_rc]) + (p.n_b * np.sum(gamma[n_rc:]))
            ss_eff[j] = nu_e / sumg
        ss_eff_mean = np.mean(ss_eff)
        ss_eff_err = np.std(ss_eff) / np.sqrt(len(subpop))
        output[rct] = (ss_eff_mean, ss_eff_err)
    return output, []

def do_stats(df, spectrum, prefix, **kwargs):
    '''
    wrapper to actually do the stats. Depending on what you want to do,
    there might be output PDFs, or just numbers that you then add to a list
    (if you're looping over some other parameter or whatever), so return
    a dict of the numerical results and a list of the generated output files.
    The way I use this is by using the dicts below and calling e.g.
    `do_stats(df, spectrum, prefix, **minimal_stats)`
    '''
    spd = {'spectrum': spectrum, 'prefix': prefix}
    output = {} # list of all output files (will be zipped)
    output_files = []
    for k, v in kwargs.items():
        fn = getattr(sys.modules[__name__], v['function'])
        # fn_kwargs = v | spd
        fn_kwargs = {**v, **spd}
        fn_kwargs['key'] = k
        op, ofs = fn(df, **fn_kwargs)
        output[k] = op
        output_files.extend(ofs)
    return output, output_files

minimal_stats = {
        'nu_e':    {'function': 'avg'},
        'fitness': {'function': 'avg'},
        'n_b':     {'function': 'avg'},
        'n_s':     {'function': 'avg'},
        'rc':      {'function': 'counts'},
    }

big_stats = {
        'shift':      {'function': 'hist', 'split': 'rc'},
        'redox':      {'function': 'hist', 'split': 'rc'},
        'nu_e':       {'function': 'hist', 'split': 'rc'},
        'n_b':        {'function': 'hist'},
        'rc':         {'function': 'counts'},
        'absorption': {'function': 'absorption', 'split': 'rc'}
    }

paper_stats = {
        'nu_e':       {'function': 'avg',           'split': 'rc'},
        'fitness':    {'function': 'avg',           'split': 'rc'},
        'redox':      {'function': 'element_avg',   'split': 'rc'},
        'n_p':        {'function': 'avg_sum',       'split': 'rc'},
        'absorption': {'function': 'absorption',    'split': 'rc'},
        'efficiency': {'function': 'ss_efficiency', 'split': 'rc'}
    }

if __name__ == "__main__":
    import light
    import solvers
    import genetic_algorithm as ga

    rng = np.random.default_rng()
    cost = 0.01
    n = constants.population_size
    spectrum, output_prefix = light.spectrum_setup("stellar",
            Tstar=5770, Rstar=6.957E8, a=1.0, attenuation=0.0)
    prefix = os.path.join(constants.output_dir, "tests", "stats",
            output_prefix)
    os.makedirs(os.path.dirname(prefix), exist_ok=True)
    population = [ga.new(rng, **{'n_b': 1, 'n_s': 1}) for _ in range(n)]
    results = {'nu_e': [], 'nu_cyc': [], 'fitness': [],
               'redox': [], 'recomb': []}
    for p in population:
        res, fail = solvers.antenna_RC(p, spectrum, nnls='scipy')
        res['fitness'] = ga.fitness(p, res['nu_e'], cost)
        for k, v in res.items():
            results[k].append(v)
    df = pd.DataFrame(population)
    for k, v in results.items():
        df[k] = v
    df.to_csv(f"{prefix}_df.csv", sep="\t")
    output, output_files = do_stats(df, spectrum, prefix, **minimal_stats)
    print("Minimal stats:")
    print(f"Output dict: {output}")
    print(f"Output file dict: {output_files}")
    output, output_files = do_stats(df, spectrum, prefix, **big_stats)
    print("Big stats:")
    print(f"Output dict: {output}")
    print(f"Output file dict: {output_files}")
